{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Preprocessing_And_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BTAaJSHd7Q"
      },
      "source": [
        "Text Preprocessing\r\n",
        "\r\n",
        "Created by: \"Mohamed Wacef Chachia\"\r\n",
        "\r\n",
        "This Notebook is a collection of functions and methods of processing text data. It saves you a lot of time in a competition and helps you focus more on building a good model with high precision\r\n",
        "\r\n",
        "These methods are not applied to specific data, so you have to make small changes to make them compatible with your data\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-G12xw4rqpq"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "from nltk.tokenize import TweetTokenizer\r\n",
        "import datetime\r\n",
        "import lightgbm as lgb\r\n",
        "from scipy import stats\r\n",
        "from scipy.sparse import hstack, csr_matrix\r\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\r\n",
        "from wordcloud import WordCloud\r\n",
        "from collections import Counter\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.util import ngrams\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.preprocessing import StandardScaler\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "from sklearn.svm import LinearSVC\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "pd.set_option('max_colwidth',400)\r\n",
        "\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\r\n",
        "from sklearn.manifold import TSNE\r\n",
        "\r\n",
        "# Keras\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\r\n",
        "from keras import utils\r\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\r\n",
        "\r\n",
        "# nltk\r\n",
        "import nltk\r\n",
        "from  nltk.stem import SnowballStemmer\r\n",
        "\r\n",
        "# Word2vec\r\n",
        "import gensim\r\n",
        "\r\n",
        "# Utility\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "from collections import Counter\r\n",
        "import logging\r\n",
        "import time\r\n",
        "import pickle\r\n",
        "import itertools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTmljPUfr3vF"
      },
      "source": [
        "!pip install gensim --upgrade\r\n",
        "!pip install keras --upgrade\r\n",
        "!pip install pandas --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgA123QIr9GO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqJ0EUD5sh6E"
      },
      "source": [
        "# KERAS\r\n",
        "SEQUENCE_LENGTH = 300\r\n",
        "EPOCHS = 8\r\n",
        "BATCH_SIZE = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3mH__eUsnj_"
      },
      "source": [
        "#target counter\r\n",
        "target_cnt = Counter(df.target)\r\n",
        "\r\n",
        "plt.figure(figsize=(16,8))\r\n",
        "plt.bar(target_cnt.keys(), target_cnt.values())\r\n",
        "plt.title(\"Dataset labels distribuition\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMwGtoaJtJ37"
      },
      "source": [
        "stop_words = stopwords.words(\"english\")\r\n",
        "stemmer = SnowballStemmer(\"english\")\r\n",
        "\r\n",
        "def preprocess(text, stem=False):\r\n",
        "    # Remove link,user and special characters\r\n",
        "    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\r\n",
        "    tokens = []\r\n",
        "    for token in text.split():\r\n",
        "        if token not in stop_words:\r\n",
        "            if stem:\r\n",
        "                tokens.append(stemmer.stem(token))\r\n",
        "            else:\r\n",
        "                tokens.append(token)\r\n",
        "    return \" \".join(tokens)\r\n",
        "\r\n",
        "df.text = df.text.apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3n9lIMftRkO"
      },
      "source": [
        "#Word2Vec\r\n",
        "# WORD2VEC \r\n",
        "W2V_SIZE = 300\r\n",
        "W2V_WINDOW = 7\r\n",
        "W2V_EPOCH = 32\r\n",
        "W2V_MIN_COUNT = 10\r\n",
        "\r\n",
        "\r\n",
        "documents = [_text.split() for _text in df_train.text] \r\n",
        "w2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \r\n",
        "                                            window=W2V_WINDOW, \r\n",
        "                                            min_count=W2V_MIN_COUNT, \r\n",
        "                                            workers=8)\r\n",
        "w2v_model.build_vocab(documents)\r\n",
        "\r\n",
        "words = w2v_model.wv.vocab.keys()\r\n",
        "vocab_size = len(words)\r\n",
        "print(\"Vocab size\", vocab_size)\r\n",
        "w2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)\r\n",
        "\r\n",
        "w2v_model.most_similar(\"love\") #exp d'un mot\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH8fyO-KuI8l"
      },
      "source": [
        "#tokenize text\r\n",
        "\r\n",
        "tokenizer = Tokenizer()\r\n",
        "tokenizer.fit_on_texts(df_train.text)\r\n",
        "\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print(\"Total words\", vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osuQ9zDSvBZJ"
      },
      "source": [
        "#exp : modéle 1\r\n",
        "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\r\n",
        "for word, i in tokenizer.word_index.items():\r\n",
        "  if word in w2v_model.wv:\r\n",
        "    embedding_matrix[i] = w2v_model.wv[word]\r\n",
        "print(embedding_matrix.shape)\r\n",
        "\r\n",
        "embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(embedding_layer)\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "\r\n",
        "model.summary()\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy',\r\n",
        "              optimizer=\"adam\",\r\n",
        "              metrics=['accuracy'])  #classification binaire\r\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZUyU19Fvf-5"
      },
      "source": [
        "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\r\n",
        "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\r\n",
        "\r\n",
        "history = model.fit(x_train, y_train,\r\n",
        "                    batch_size=BATCH_SIZE,\r\n",
        "                    epochs=EPOCHS,\r\n",
        "                    validation_split=0.1,\r\n",
        "                    verbose=1,\r\n",
        "                    callbacks=callbacks)\r\n",
        "\r\n",
        "score = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\r\n",
        "print()\r\n",
        "print(\"ACCURACY:\",score[1])\r\n",
        "print(\"LOSS:\",score[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf-EvK3azePc"
      },
      "source": [
        "#predict modéle 1\r\n",
        "\r\n",
        "def predict(text, include_neutral=True):\r\n",
        "    start_at = time.time()\r\n",
        "    # Tokenize text\r\n",
        "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\r\n",
        "    # Predict\r\n",
        "    score = model.predict([x_test])[0]\r\n",
        "    # Decode sentiment\r\n",
        "    label = decode_sentiment(score, include_neutral=include_neutral)\r\n",
        "\r\n",
        "    return {\"label\": label, \"score\": float(score),\r\n",
        "       \"elapsed_time\": time.time()-start_at}  \r\n",
        "\r\n",
        "predict(\"I love the music\") #exp1\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z7Y7NCz1LgK"
      },
      "source": [
        "#save model1\r\n",
        "model.save(KERAS_MODEL)\r\n",
        "w2v_model.save(WORD2VEC_MODEL)\r\n",
        "pickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\r\n",
        "pickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyCGUtuE1ZuG"
      },
      "source": [
        "########################################### Fin Modéle 1 ###########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRz_uFWq4nJT"
      },
      "source": [
        "#autres exp des opérations possibles :\r\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\r\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfk5Oq86h-h"
      },
      "source": [
        "max_features = 2000\r\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\r\n",
        "tokenizer.fit_on_texts(data['text'].values)\r\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\r\n",
        "X = pad_sequences(X)\r\n",
        "\r\n",
        "#exp2 of LSTM Network\r\n",
        "\r\n",
        "embed_dim = 128\r\n",
        "lstm_out = 196\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(max_fatures, embed_dim,input_length = X.shape[1]))\r\n",
        "model.add(SpatialDropout1D(0.4))\r\n",
        "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(2,activation='softmax'))\r\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\r\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YR24Gmr6sio"
      },
      "source": [
        "batch_size = 32\r\n",
        "model.fit(X_train, Y_train, epochs = 15, batch_size=batch_size, verbose = 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SUAImUi6tgX"
      },
      "source": [
        "score,acc = model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\r\n",
        "print(\"score: %.2f\" % (score))\r\n",
        "print(\"acc: %.2f\" % (acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28HP3zoD9a1x"
      },
      "source": [
        "######################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPf6-IZt9i16"
      },
      "source": [
        "# This function converts a text to a sequence of words.\r\n",
        "def review_wordlist(review, remove_stopwords=False):\r\n",
        "    # 1. Removing html tags\r\n",
        "    review_text = BeautifulSoup(review).get_text()\r\n",
        "    # 2. Removing non-letter.\r\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\r\n",
        "    # 3. Converting to lower case and splitting\r\n",
        "    words = review_text.lower().split()\r\n",
        "    # 4. Optionally remove stopwords\r\n",
        "    if remove_stopwords:\r\n",
        "        stops = set(stopwords.words(\"english\"))     \r\n",
        "        words = [w for w in words if not w in stops]\r\n",
        "    \r\n",
        "    return(words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKGWGty8EOmu"
      },
      "source": [
        "import nltk.data\r\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
        "\r\n",
        "# This function splits a review into sentences\r\n",
        "def review_sentences(review, tokenizer, remove_stopwords=False):\r\n",
        "    # 1. Using nltk tokenizer\r\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\r\n",
        "    sentences = []\r\n",
        "    # 2. Loop for each sentence\r\n",
        "    for raw_sentence in raw_sentences:\r\n",
        "        if len(raw_sentence)>0:\r\n",
        "            sentences.append(review_wordlist(raw_sentence,\\\r\n",
        "                                            remove_stopwords))\r\n",
        "\r\n",
        "    # This returns the list of lists\r\n",
        "    return sentences\r\n",
        "\r\n",
        "\r\n",
        "sentences = []\r\n",
        "print(\"Parsing sentences from training set\")\r\n",
        "for review in train[\"review\"]:\r\n",
        "    sentences += review_sentences(review, tokenizer)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXgrF9rWEmu5"
      },
      "source": [
        "#Word2vec model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2d6ZnBXEnit"
      },
      "source": [
        "# Importing the built-in logging module\r\n",
        "import logging\r\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\r\n",
        "\r\n",
        "# Creating the model and setting values for the various parameters\r\n",
        "num_features = 300  # Word vector dimensionality\r\n",
        "min_word_count = 40 # Minimum word count\r\n",
        "num_workers = 4     # Number of parallel threads\r\n",
        "context = 10        # Context window size\r\n",
        "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\r\n",
        "\r\n",
        "# Initializing the train model\r\n",
        "from gensim.models import word2vec\r\n",
        "print(\"Training model....\")\r\n",
        "model = word2vec.Word2Vec(sentences,\\\r\n",
        "                          workers=num_workers,\\\r\n",
        "                          size=num_features,\\\r\n",
        "                          min_count=min_word_count,\\\r\n",
        "                          window=context,\r\n",
        "                          sample=downsampling)\r\n",
        "\r\n",
        "# To make the model memory efficient\r\n",
        "model.init_sims(replace=True)\r\n",
        "\r\n",
        "# Saving the model for later use. Can be loaded using Word2Vec.load()\r\n",
        "model_name = \"300features_40minwords_10context\"\r\n",
        "model.save(model_name)\r\n",
        "\r\n",
        "# Few tests: This will print the odd word among them \r\n",
        "model.wv.doesnt_match(\"man woman dog child kitchen\".split())\r\n",
        "\r\n",
        "model.wv.doesnt_match(\"france england germany berlin\".split())\r\n",
        "\r\n",
        "# This will print the most similar words present in the model\r\n",
        "model.wv.most_similar(\"man\")\r\n",
        "\r\n",
        "model.wv.most_similar(\"awful\")\r\n",
        "\r\n",
        "# This will give the total number of words in the vocabolary created from this dataset\r\n",
        "model.wv.syn0.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_hK4ApKHcZH"
      },
      "source": [
        "##########################################################\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NT5JerYHcTv"
      },
      "source": [
        " #Tokenization of text\r\n",
        "tokenizer=ToktokTokenizer()\r\n",
        "#Setting English stopwords\r\n",
        "stopword_list=nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQrgDwWgHcLY"
      },
      "source": [
        "#Removing the html strips\r\n",
        "def strip_html(text):\r\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\r\n",
        "    return soup.get_text()\r\n",
        "\r\n",
        "#Removing the square brackets\r\n",
        "def remove_between_square_brackets(text):\r\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\r\n",
        "\r\n",
        "#Removing the noisy text\r\n",
        "def denoise_text(text):\r\n",
        "    text = strip_html(text)\r\n",
        "    text = remove_between_square_brackets(text)\r\n",
        "    return text\r\n",
        "#Apply function on review column\r\n",
        "imdb_data['review']=imdb_data['review'].apply(denoise_text)\r\n",
        "\r\n",
        "#Define function for removing special characters\r\n",
        "def remove_special_characters(text, remove_digits=True):\r\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\r\n",
        "    text=re.sub(pattern,'',text)\r\n",
        "    return text\r\n",
        "#Apply function on review column\r\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)\r\n",
        "\r\n",
        "#Stemming the text\r\n",
        "def simple_stemmer(text):\r\n",
        "    ps=nltk.porter.PorterStemmer()\r\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\r\n",
        "    return text\r\n",
        "#Apply function on review column\r\n",
        "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)\r\n",
        "\r\n",
        "#set stopwords to english\r\n",
        "stop=set(stopwords.words('english'))\r\n",
        "print(stop)\r\n",
        "\r\n",
        "#removing the stopwords\r\n",
        "def remove_stopwords(text, is_lower_case=False):\r\n",
        "    tokens = tokenizer.tokenize(text)\r\n",
        "    tokens = [token.strip() for token in tokens]\r\n",
        "    if is_lower_case:\r\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\r\n",
        "    else:\r\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\r\n",
        "    filtered_text = ' '.join(filtered_tokens)    \r\n",
        "    return filtered_text\r\n",
        "#Apply function on review column\r\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-St_FH2O2UB"
      },
      "source": [
        "#Bags of words model\r\n",
        "#It is used to convert text documents to numerical vectors or bag of words.\r\n",
        "\r\n",
        "#Count vectorizer for bag of words\r\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\r\n",
        "#transformed test reviews\r\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\r\n",
        "\r\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\r\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)\r\n",
        "#vocab=cv.get_feature_names()-toget feature names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZIyfktbPChG"
      },
      "source": [
        "#Tfidf vectorizer\r\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\r\n",
        "#transformed train reviews\r\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\r\n",
        "#transformed test reviews\r\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\r\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\r\n",
        "print('Tfidf_test:',tv_test_reviews.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwez1ay-PJsQ"
      },
      "source": [
        "#labeling the sentient data\r\n",
        "lb=LabelBinarizer()\r\n",
        "#transformed sentiment data\r\n",
        "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\r\n",
        "print(sentiment_data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYd__QwAWKEt"
      },
      "source": [
        "###############################################################\"\r\n",
        "#autre exp :"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_5h8bugWOgl"
      },
      "source": [
        "tokenizer = TweetTokenizer()\r\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), tokenizer=tokenizer.tokenize)\r\n",
        "full_text = list(train['Phrase'].values) + list(test['Phrase'].values)\r\n",
        "vectorizer.fit(full_text)\r\n",
        "train_vectorized = vectorizer.transform(train['Phrase'])\r\n",
        "test_vectorized = vectorizer.transform(test['Phrase'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn1iCx3KWOSl"
      },
      "source": [
        "#Deep learning\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\r\n",
        "from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\r\n",
        "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\r\n",
        "from keras.models import Model, load_model\r\n",
        "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\r\n",
        "from keras import backend as K\r\n",
        "from keras.engine import InputSpec, Layer\r\n",
        "from keras.optimizers import Adam\r\n",
        "\r\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMIvR0nqWcA3"
      },
      "source": [
        "tk = Tokenizer(lower = True, filters='')\r\n",
        "tk.fit_on_texts(full_text)\r\n",
        "\r\n",
        "train_tokenized = tk.texts_to_sequences(train['Phrase'])\r\n",
        "test_tokenized = tk.texts_to_sequences(test['Phrase'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBKwdHYiWiBY"
      },
      "source": [
        "max_len = 50\r\n",
        "X_train = pad_sequences(train_tokenized, maxlen = max_len)\r\n",
        "X_test = pad_sequences(test_tokenized, maxlen = max_len)\r\n",
        "embedding_path = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\r\n",
        "embed_size = 300\r\n",
        "max_features = 30000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlsAEzjEWr8q"
      },
      "source": [
        "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\r\n",
        "embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\r\n",
        "\r\n",
        "word_index = tk.word_index\r\n",
        "nb_words = min(max_features, len(word_index))\r\n",
        "embedding_matrix = np.zeros((nb_words + 1, embed_size))\r\n",
        "for word, i in word_index.items():\r\n",
        "    if i >= max_features: continue\r\n",
        "    embedding_vector = embedding_index.get(word)\r\n",
        "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVXhztcNWwFS"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "ohe = OneHotEncoder(sparse=False)\r\n",
        "y_ohe = ohe.fit_transform(y.values.reshape(-1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1tXXaIJW0ey"
      },
      "source": [
        "def build_model1(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\r\n",
        "    file_path = \"best_model.hdf5\"\r\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\r\n",
        "                                  save_best_only = True, mode = \"min\")\r\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\r\n",
        "    \r\n",
        "    inp = Input(shape = (max_len,))\r\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\r\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\r\n",
        "\r\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\r\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\r\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x1)\r\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x1)\r\n",
        "    \r\n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\r\n",
        "    avg_pool3_gru = GlobalAveragePooling1D()(x3)\r\n",
        "    max_pool3_gru = GlobalMaxPooling1D()(x3)\r\n",
        "    \r\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\r\n",
        "    x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\r\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\r\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x1)\r\n",
        "    \r\n",
        "    x3 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\r\n",
        "    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\r\n",
        "    max_pool3_lstm = GlobalMaxPooling1D()(x3)\r\n",
        "    \r\n",
        "    \r\n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\r\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\r\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\r\n",
        "    model = Model(inputs = inp, outputs = x)\r\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\r\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \r\n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\r\n",
        "    model = load_model(file_path)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-kYROh1W6lr"
      },
      "source": [
        "model1 = build_model1(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KmVWLybW6ge"
      },
      "source": [
        "#modéle12 plus mieux au niveau de \"loss\"\r\n",
        "\r\n",
        "def build_model2(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\r\n",
        "    file_path = \"best_model.hdf5\"\r\n",
        "    check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\r\n",
        "                                  save_best_only = True, mode = \"min\")\r\n",
        "    early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\r\n",
        "\r\n",
        "    inp = Input(shape = (max_len,))\r\n",
        "    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\r\n",
        "    x1 = SpatialDropout1D(spatial_dr)(x)\r\n",
        "\r\n",
        "    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\r\n",
        "    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\r\n",
        "    \r\n",
        "    x_conv1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\r\n",
        "    avg_pool1_gru = GlobalAveragePooling1D()(x_conv1)\r\n",
        "    max_pool1_gru = GlobalMaxPooling1D()(x_conv1)\r\n",
        "    \r\n",
        "    x_conv2 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_gru)\r\n",
        "    avg_pool2_gru = GlobalAveragePooling1D()(x_conv2)\r\n",
        "    max_pool2_gru = GlobalMaxPooling1D()(x_conv2)\r\n",
        "    \r\n",
        "    \r\n",
        "    x_conv3 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\r\n",
        "    avg_pool1_lstm = GlobalAveragePooling1D()(x_conv3)\r\n",
        "    max_pool1_lstm = GlobalMaxPooling1D()(x_conv3)\r\n",
        "    \r\n",
        "    x_conv4 = Conv1D(conv_size, kernel_size=kernel_size2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\r\n",
        "    avg_pool2_lstm = GlobalAveragePooling1D()(x_conv4)\r\n",
        "    max_pool2_lstm = GlobalMaxPooling1D()(x_conv4)\r\n",
        "    \r\n",
        "    \r\n",
        "    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool2_gru, max_pool2_gru,\r\n",
        "                    avg_pool1_lstm, max_pool1_lstm, avg_pool2_lstm, max_pool2_lstm])\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\r\n",
        "    x = BatchNormalization()(x)\r\n",
        "    x = Dropout(dr)(Dense(int(dense_units / 2), activation='relu') (x))\r\n",
        "    x = Dense(5, activation = \"sigmoid\")(x)\r\n",
        "    model = Model(inputs = inp, outputs = x)\r\n",
        "    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\r\n",
        "    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 20, validation_split=0.1, \r\n",
        "                        verbose = 1, callbacks = [check_point, early_stop])\r\n",
        "    model = load_model(file_path)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca8xWVe9W6Z9"
      },
      "source": [
        "model4 = build_model2(lr = 1e-3, lr_d = 0, units = 64, spatial_dr = 0.5, kernel_size1=3, kernel_size2=3, dense_units=64, dr=0.3, conv_size=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4DXu3xuW6Se"
      },
      "source": [
        "#prediction\r\n",
        "pred1 = model1.predict(X_test, batch_size = 1024, verbose = 1)\r\n",
        "predictions = np.round(np.argmax(pred1, axis=1)).astype(int)\r\n",
        "sub['Sentiment'] = predictions\r\n",
        "sub.to_csv(\"submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEp3a2HxX2AR"
      },
      "source": [
        "#########################################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3tbEMCxX4DQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}